{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "from collections import OrderedDict\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from scipy import io as sio\n",
    "import sklearn.metrics as sm\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from keras.utils import np_utils\n",
    "\n",
    "# TCN imports \n",
    "import tf_models, ap_datasets, utils, metrics\n",
    "from utils import imshow_\n",
    "\n",
    "# ---------- Directories & User inputs --------------\n",
    "# Location of data/features folder\n",
    "base_dir = os.path.expanduser(\"../\")\n",
    "\n",
    "save_predictions = [False, True][1]\n",
    "viz_predictions = [False, True][1]\n",
    "viz_weights = [False, True][0]\n",
    "\n",
    "# Set dataset and action label granularity (if applicable)\n",
    "dataset = [\"50Salads\", \"JIGSAWS\", \"MERL\", \"GTEA\", \"UCF101\"][4]\n",
    "granularity = [\"eval\", \"mid\"][1]\n",
    "sensor_type = [\"video\", \"sensors\"][0]\n",
    "\n",
    "# Set model and parameters\n",
    "model_type = [\"ED-TCN\",\"AP-TCN\"][1]\n",
    "# causal or acausal? (If acausal use Bidirectional LSTM)\n",
    "causal = [False, True][0]\n",
    "\n",
    "# How many latent states/nodes per layer of network\n",
    "# Only applicable to the TCNs. The ECCV and LSTM  model suses the first element from this list.\n",
    "n_nodes = [64, 96]\n",
    "nb_epoch = 50 #50\n",
    "video_rate = 3\n",
    "conv = {'50Salads':25, \"JIGSAWS\":20, \"MERL\":5, \"GTEA\":25, 'UCF101': 25}[dataset]\n",
    "\n",
    "# Which features for the given dataset\n",
    "features = \"SpatialCNN\"\n",
    "bg_class = 0 if dataset is not \"JIGSAWS\" else None\n",
    "\n",
    "if dataset == \"50Salads\":\n",
    "    features = \"SpatialCNN_\" + granularity\n",
    "if dataset == \"UCF101\":\n",
    "    base_dir = \"/home/jinchoi/src/rehab/dataset/action/UCF101/\"\n",
    "    feature_type = 'relu7_feat'\n",
    "    video_rate = 25\n",
    "\n",
    "data = ap_datasets.Dataset(dataset, base_dir, feature_type='fc7')\n",
    "trial_metrics = metrics.ComputeMetrics(overlap=.1, bg_class=bg_class)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data split...\n",
      "npy files found.\n",
      "Loading done.\n",
      "Start data masking...\n",
      "Data masking done.\n",
      "Training AP TCN...\n",
      "Epoch 1/50\n",
      "8160/9537 [========================>.....] - ETA: 13s - loss: 4.3707 - acc: 0.0369"
     ]
    }
   ],
   "source": [
    "#from IPython.core.debugger import Pdb # for debugging, jinchoi@vt.edu\n",
    "#pdb = Pdb()\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# Evaluate using different filter lengths\n",
    "if 1:\n",
    "# for conv in [5, 10, 15, 20]:\n",
    "    # Initialize dataset loader & metrics\n",
    "    if dataset == 'UCF101':\n",
    "        data = ap_datasets.Dataset(dataset, base_dir, feature_type='fc7')\n",
    "    else:\n",
    "        data = ap_datasets.Dataset(dataset, base_dir)\n",
    "\n",
    "    split_cnt = 0\n",
    "    accuracies = np.zeros(len(data.splits))\n",
    "    train_accuracies = np.zeros(len(data.splits))\n",
    "    \n",
    "    # Load data for each split\n",
    "    for split in data.splits:\n",
    "        if dataset != 'UCF101':\n",
    "            if sensor_type==\"video\":\n",
    "                feature_type = \"A\" if model_type != \"SVM\" else \"X\"\n",
    "            else:\n",
    "                feature_type = \"S\"\n",
    "        \n",
    "        print(\"Loading data split...\")\n",
    "        if ( os.path.exists(base_dir + 'AlexNet-fc7-npy/X_train_ucf_' + split + '.npy') and \n",
    "             os.path.exists(base_dir + 'AlexNet-fc7-npy/y_train_ucf_' + split + '.npy') and\n",
    "             os.path.exists(base_dir + 'AlexNet-fc7-npy/X_test_ucf_' + split + '.npy') and\n",
    "             os.path.exists(base_dir + 'AlexNet-fc7-npy/y_test_ucf_' + split + '.npy') ):\n",
    "            print(\"npy files found.\")\n",
    "            X_train = np.load(base_dir + 'AlexNet-fc7-npy/X_train_ucf_' + split + '.npy');\n",
    "            y_train = np.load(base_dir + 'AlexNet-fc7-npy/y_train_ucf_' + split + '.npy');\n",
    "            X_test = np.load(base_dir + 'AlexNet-fc7-npy/X_test_ucf_' + split + '.npy');\n",
    "            y_test = np.load(base_dir + 'AlexNet-fc7-npy/y_test_ucf_' + split + '.npy');\n",
    "        else:\n",
    "            print(\"npy files not found. Loading from mat files. This would take a while...\")\n",
    "            X_train, y_train, X_test, y_test = data.load_split(features, split=split, \n",
    "                                                                sample_rate=video_rate, \n",
    "                                                                feature_type=feature_type)\n",
    "            np.save(base_dir + 'AlexNet-fc7-npy/X_train_ucf_' + split + '.npy', X_train);\n",
    "            np.save(base_dir + 'AlexNet-fc7-npy/y_train_ucf_' + split + '.npy', y_train);\n",
    "            np.save(base_dir + 'AlexNet-fc7-npy/X_test_ucf_' + split + '.npy', X_test);\n",
    "            np.save(base_dir + 'AlexNet-fc7-npy/y_test_ucf_' + split + '.npy', y_test);\n",
    "        print(\"Loading done.\")\n",
    "        \n",
    "#         pca = PCA(n_components = 32)\n",
    "#         pca.fit(X_train[0])\n",
    "\n",
    "        n_classes = data.n_classes\n",
    "        n_train = len(X_train)\n",
    "        n_test = len(X_test)\n",
    "\n",
    "        n_feat = data.n_features = X_train[0].shape[1]\n",
    "\n",
    "        # --------- CVPR model ----------\n",
    "        if model_type in [\"ED-TCN\",\"AP-TCN\"]:\n",
    "            # for train a softmax classfier, we need one-hot encoded labels, from 0 to 100\n",
    "            Y_train = [np_utils.to_categorical(y-1, n_classes) for y in y_train]            \n",
    "            y_train_ = [np.array([y_train[i]]) for i in range(len(y_train))]\n",
    "            \n",
    "            # for train a softmax classfier, we need one-hot encoded labels, from 0 to 100\n",
    "            y_test_ = [np.array([y_test[i]]) for i in range(len(y_test))]\n",
    "            \n",
    "            # In order process batches simultaneously all data needs to be of the same length\n",
    "            # So make all same length and mask out the ends of each.\n",
    "            n_layers = len(n_nodes)\n",
    "            max_len = 90 # this should be elaborated\n",
    "            \n",
    "            print(\"Start data masking...\")\n",
    "            X_train_m, M_train = utils.mask_data_one_tensor(X_train, max_len, mask_value=-1)\n",
    "            X_test_m, M_test = utils.mask_data_one_tensor(X_test, max_len, mask_value=-1)\n",
    "            Y_train_ = np.array(Y_train)            \n",
    "            print(\"Data masking done.\")\n",
    "                       \n",
    "            if model_type == \"ED-TCN\":\n",
    "                model, param_str = tf_models.ED_TCN(n_nodes, conv, n_classes, n_feat, max_len, causal=causal, activation='norm_relu', return_param_str=True) \n",
    "            elif model_type == \"AP-TCN\":\n",
    "                print 'Training AP TCN...'\n",
    "                model, param_str = tf_models.AP_TCN(n_nodes, conv, n_classes, n_feat, max_len, causal=causal, \n",
    "                                        activation='norm_relu', return_param_str=True)               \n",
    "            \n",
    "            # summarize the model connection and compilation\n",
    "            #model.summary()\n",
    "            M_train2 = np.array([[1] for i in range(len(X_train))])\n",
    "            \n",
    "            # fitting a model\n",
    "            model.fit(X_train_m, Y_train_, nb_epoch=nb_epoch, batch_size=8, verbose=1, sample_weight=M_train2) \n",
    "            \n",
    "            # predict on the dataset\n",
    "            print(\"Prediction on training data...\")\n",
    "            AP_train = model.predict(X_train_m, verbose=0)\n",
    "            print(\"Prediction on test data...\")\n",
    "            AP_test = model.predict(X_test_m, verbose=0)\n",
    "            print(\"All predictions done...\")\n",
    "            \n",
    "            # prediction outputs from 0 to 100 while GT contains labels from 1 to 101\n",
    "            P_train = [p.argmax(1)+1 for p in AP_train] \n",
    "            P_test = [p.argmax(1)+1 for p in AP_test]\n",
    " \n",
    "        else:\n",
    "            print(\"Model not available:\", model_type)\n",
    "\n",
    "        print(param_str)\n",
    "\n",
    "        cnt = 0\n",
    "        for i in range(len(P_test)):\n",
    "            if P_test[i][0] == y_test_[i][0]:\n",
    "                cnt += 1\n",
    "        accuracy = float(cnt)/len(P_test)\n",
    "        print 'Test accuracy on {0}: {1}'.format(split, accuracy)\n",
    "        accuracies[split_cnt] = accuracy\n",
    "        \n",
    "        cnt = 0\n",
    "        for i in range(len(P_train)):\n",
    "            if P_train[i][0] == y_train_[i][0]:\n",
    "                cnt += 1\n",
    "        train_accuracy = float(cnt)/len(P_train)\n",
    "        print 'Training accuracy on {0}: {1}'.format(split, train_accuracy)\n",
    "        train_accuracies[split_cnt] = train_accuracy\n",
    "        \n",
    "        split_cnt += 1\n",
    "        \n",
    "        if split_cnt >= 1:\n",
    "            break\n",
    "        \n",
    "    print 'Mean accuracy: {0}'.format(np.mean(accuracies))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:tf-ap]",
   "language": "python",
   "name": "conda-env-tf-ap-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
