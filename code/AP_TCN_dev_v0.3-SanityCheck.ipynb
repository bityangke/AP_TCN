{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "from collections import OrderedDict\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from scipy import io as sio\n",
    "import sklearn.metrics as sm\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from keras.utils import np_utils\n",
    "\n",
    "# TCN imports \n",
    "import tf_models, ap_datasets, utils, metrics\n",
    "from utils import imshow_\n",
    "\n",
    "# ---------- Directories & User inputs --------------\n",
    "# Location of data/features folder\n",
    "base_dir = os.path.expanduser(\"../\")\n",
    "\n",
    "save_predictions = [False, True][1]\n",
    "viz_predictions = [False, True][1]\n",
    "viz_weights = [False, True][0]\n",
    "\n",
    "# Set dataset and action label granularity (if applicable)\n",
    "dataset = [\"50Salads\", \"JIGSAWS\", \"MERL\", \"GTEA\", \"UCF101\"][4]\n",
    "\n",
    "# Set model and parameters\n",
    "model_type = [\"AP-TCN\"][0]\n",
    "# causal or acausal? (If acausal use Bidirectional LSTM)\n",
    "causal = [False, True][0]\n",
    "\n",
    "# How many latent states/nodes per layer of network\n",
    "# Only applicable to the TCNs. The ECCV and LSTM  model suses the first element from this list.\n",
    "n_nodes = [64, 96]\n",
    "nb_epoch = 3 #50\n",
    "video_rate = 3\n",
    "conv = {'50Salads':25, \"JIGSAWS\":20, \"MERL\":5, \"GTEA\":25, 'UCF101': 25}[dataset]\n",
    "\n",
    "# Which features for the given dataset\n",
    "features = \"SpatialCNN\"\n",
    "\n",
    "if dataset == \"UCF101\":\n",
    "    base_dir = \"/home/jinchoi/src/rehab/dataset/action/UCF101/\"\n",
    "    feature_type = 'relu7_feat'\n",
    "    video_rate = 25\n",
    "    architecture = 'sanity_check1'\n",
    "    # In order process batches simultaneously all data needs to be of the same length\n",
    "    # So make all same length and mask out the ends of each.\n",
    "    max_len = 90 # this should be elaborated\n",
    "    \n",
    "data = ap_datasets.Dataset(dataset, base_dir, feature_type='fc7')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data split...\n",
      "npy files found.\n",
      "Loading done.\n",
      "Start data masking...\n",
      "Data masking done.\n",
      "Training AP TCN...\n",
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input_1 (InputLayer)             (None, 90, 4096)      0                                            \n",
      "____________________________________________________________________________________________________\n",
      "maxpooling1d_1 (MaxPooling1D)    (None, 1, 4096)       0           input_1[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)              (None, 4096)          0           maxpooling1d_1[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "repeatvector_1 (RepeatVector)    (None, 1, 4096)       0           flatten_1[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dense_1 (Dense)                  (None, 1, 101)        413797      repeatvector_1[0][0]             \n",
      "====================================================================================================\n",
      "Total params: 413,797\n",
      "Trainable params: 413,797\n",
      "Non-trainable params: 0\n",
      "____________________________________________________________________________________________________\n",
      "Train on 8583 samples, validate on 954 samples\n",
      "Epoch 1/3\n",
      "2048/8583 [======>.......................] - ETA: 37s - loss: 3.8534 - acc: 0.1382"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-235c59f9856e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     86\u001b[0m             \u001b[0;31m# fitting a model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[0;31m#             model.fit(X_train_m_shuffle, Y_train_shuffle, nb_epoch=nb_epoch, batch_size=8, verbose=1, shuffle=True, sample_weight=M_train2)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m             \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_m\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_train_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnb_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnb_epoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mM_train2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m             \u001b[0;31m# predict on the dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/jinchoi/anaconda2/envs/tf-ap/lib/python2.7/site-packages/Keras-1.2.1-py2.7.egg/keras/engine/training.pyc\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, nb_epoch, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch)\u001b[0m\n\u001b[1;32m   1195\u001b[0m                               \u001b[0mval_f\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_f\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_ins\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_ins\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m                               \u001b[0mcallback_metrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallback_metrics\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1197\u001b[0;31m                               initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1199\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/jinchoi/anaconda2/envs/tf-ap/lib/python2.7/site-packages/Keras-1.2.1-py2.7.egg/keras/engine/training.pyc\u001b[0m in \u001b[0;36m_fit_loop\u001b[0;34m(self, f, ins, out_labels, batch_size, nb_epoch, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch)\u001b[0m\n\u001b[1;32m    890\u001b[0m                 \u001b[0mbatch_logs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'size'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m                 \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 892\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    893\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m                     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/jinchoi/anaconda2/envs/tf-ap/lib/python2.7/site-packages/Keras-1.2.1-py2.7.egg/keras/backend/tensorflow_backend.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   1920\u001b[0m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1921\u001b[0m         updated = session.run(self.outputs + [self.updates_op],\n\u001b[0;32m-> 1922\u001b[0;31m                               feed_dict=feed_dict)\n\u001b[0m\u001b[1;32m   1923\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1924\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/jinchoi/anaconda2/envs/tf-ap/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    764\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    765\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 766\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    767\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    768\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/jinchoi/anaconda2/envs/tf-ap/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    962\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    963\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 964\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    965\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    966\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/jinchoi/anaconda2/envs/tf-ap/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1012\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1013\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1014\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1015\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1016\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/home/jinchoi/anaconda2/envs/tf-ap/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1019\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1020\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1021\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1022\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1023\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/jinchoi/anaconda2/envs/tf-ap/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1001\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m   1002\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1003\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1004\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1005\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#from IPython.core.debugger import Pdb # for debugging, jinchoi@vt.edu\n",
    "#pdb = Pdb()\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# Evaluate using different filter lengths\n",
    "if 1:\n",
    "# for conv in [5, 10, 15, 20]:\n",
    "    # Initialize dataset loader & metrics\n",
    "    if dataset == 'UCF101':\n",
    "        data = ap_datasets.Dataset(dataset, base_dir, feature_type='fc7')\n",
    "    else:\n",
    "        data = ap_datasets.Dataset(dataset, base_dir)\n",
    "\n",
    "    split_cnt = 0\n",
    "    accuracies = np.zeros(len(data.splits))\n",
    "    train_accuracies = np.zeros(len(data.splits))\n",
    "    \n",
    "    # Load data for each split\n",
    "    for split in data.splits:\n",
    "        if dataset != 'UCF101':\n",
    "            feature_type = \"A\" if model_type != \"SVM\" else \"X\"\n",
    "\n",
    "        print(\"Loading data split...\")\n",
    "        if ( os.path.exists(base_dir + 'AlexNet-fc7-npy/X_train_ucf_' + split + '_' + architecture +'.npy') and \n",
    "             os.path.exists(base_dir + 'AlexNet-fc7-npy/y_train_ucf_' + split + '_' + architecture +'.npy') and\n",
    "             os.path.exists(base_dir + 'AlexNet-fc7-npy/X_test_ucf_' + split + '_' + architecture +'.npy') and\n",
    "             os.path.exists(base_dir + 'AlexNet-fc7-npy/y_test_ucf_' + split + '_' + architecture +'.npy') ):\n",
    "            print(\"npy files found.\")\n",
    "            X_train = np.load(base_dir + 'AlexNet-fc7-npy/X_train_ucf_' + split + '_' + architecture +'.npy')\n",
    "            y_train = np.load(base_dir + 'AlexNet-fc7-npy/y_train_ucf_' + split + '_' + architecture + '.npy')\n",
    "            X_test = np.load(base_dir + 'AlexNet-fc7-npy/X_test_ucf_' + split + '_' + architecture +'.npy')\n",
    "            y_test = np.load(base_dir + 'AlexNet-fc7-npy/y_test_ucf_' + split + '_' + architecture +'.npy')\n",
    "        else:\n",
    "            print(\"npy files not found. Loading from mat files. This would take a while...\")\n",
    "            X_train, y_train, X_test, y_test = data.load_split(features, split=split, \n",
    "                                                                sample_rate=video_rate, \n",
    "                                                                feature_type=feature_type)\n",
    "            np.save(base_dir + 'AlexNet-fc7-npy/X_train_ucf_' + split + '_' + architecture +'.npy', X_train)\n",
    "            np.save(base_dir + 'AlexNet-fc7-npy/y_train_ucf_' + split + '_' + architecture +'.npy', y_train)\n",
    "            np.save(base_dir + 'AlexNet-fc7-npy/X_test_ucf_' + split + '_' + architecture +'.npy', X_test)\n",
    "            np.save(base_dir + 'AlexNet-fc7-npy/y_test_ucf_' + split + '_' + architecture +'.npy', y_test)\n",
    "        print(\"Loading done.\")\n",
    "        \n",
    "        n_classes = data.n_classes\n",
    "        n_feat = data.n_features = X_train[0].shape[1]\n",
    "\n",
    "        # --------- ICCV model ----------\n",
    "        if model_type in [\"AP-TCN\"]:\n",
    "            # for train a softmax classfier, we need one-hot encoded labels, from 0 to 100\n",
    "            Y_train = [np_utils.to_categorical(y-1, n_classes) for y in y_train]\n",
    "            Y_train_ = np.array(Y_train)\n",
    "            y_train_ = [np.array([y_train[i]]) for i in range(len(y_train))]\n",
    "            \n",
    "            # for train a softmax classfier, we need one-hot encoded labels, from 0 to 100\n",
    "            y_test_ = [np.array([y_test[i]]) for i in range(len(y_test))]\n",
    "            \n",
    "            print(\"Start data masking...\")\n",
    "            if ( os.path.exists(base_dir + 'AlexNet-fc7-npy/X_train_m_ucf_' + split + '_' + architecture +'.npy') and \n",
    "                 os.path.exists(base_dir + 'AlexNet-fc7-npy/X_test_m_ucf_' + split + '_' + architecture +'.npy') ):\n",
    "                print(\"Found masked npy files... loading it\")\n",
    "                X_train_m = np.load(base_dir + 'AlexNet-fc7-npy/X_train_m_ucf_' + split + '_' + architecture +'.npy')\n",
    "                X_test_m = np.load(base_dir + 'AlexNet-fc7-npy/X_test_m_ucf_' + split + '_' + architecture + '.npy')\n",
    "            else:\n",
    "                print(\"Generating masked np objects... saving it\")\n",
    "                X_train_m, _ = utils.mask_data_one_tensor(X_train, max_len, mask_value=-1)\n",
    "                X_test_m, _ = utils.mask_data_one_tensor(X_test, max_len, mask_value=-1)\n",
    "                np.save(base_dir + 'AlexNet-fc7-npy/X_train_m_ucf_' + split + '_' + architecture +'.npy', X_train_m)\n",
    "                np.save(base_dir + 'AlexNet-fc7-npy/X_test_m_ucf_' + split + '_' + architecture +'.npy', X_test_m)\n",
    "            print(\"Data masking done.\")\n",
    "      \n",
    "            if model_type == \"AP-TCN\":\n",
    "                print 'Training AP TCN...'\n",
    "                model, param_str = tf_models.AP_TCN_SanityCheck(n_nodes, conv, n_classes, n_feat, max_len, causal=causal, \n",
    "                                                                activation='norm_relu', return_param_str=True) \n",
    "#                 model, param_str = tf_models.AP_TCN(n_nodes, conv, n_classes, n_feat, max_len, causal=causal, \n",
    "#                                         activation='norm_relu', return_param_str=True)               \n",
    "            \n",
    "            # summarize the model connection and compilation\n",
    "            model.summary()\n",
    "            M_train2 = np.array([[1] for i in range(len(X_train))])\n",
    "                  \n",
    "            ### Random shuffle of the training sequence and corresponding labels\n",
    "#             rand_ind = np.random.permutation(len(X_train_m))\n",
    "#             X_train_m_shuffle = X_train_m[rand_ind,:,:]\n",
    "#             Y_train_shuffle = np.array( [Y_train[rand_ind[i]] for i in range(len(Y_train))] )\n",
    "            ###\n",
    "                       \n",
    "            # fitting a model\n",
    "#             model.fit(X_train_m_shuffle, Y_train_shuffle, nb_epoch=nb_epoch, batch_size=8, verbose=1, shuffle=True, sample_weight=M_train2) \n",
    "            model.fit(X_train_m, Y_train_, nb_epoch=nb_epoch, batch_size=8, verbose=1, validation_split=0.1, shuffle=True, sample_weight=M_train2) \n",
    "            \n",
    "            # predict on the dataset\n",
    "            print(\"Prediction on training data...\")            \n",
    "            AP_train = model.predict(X_train_m, verbose=0)\n",
    "            print(\"Prediction on test data...\")\n",
    "            AP_test = model.predict(X_test_m, verbose=0)\n",
    "            print(\"All predictions done...\")\n",
    "            \n",
    "            # prediction outputs from 0 to 100 while GT contains labels from 1 to 101\n",
    "            P_train = [p.argmax(1)+1 for p in AP_train] \n",
    "            P_test = [p.argmax(1)+1 for p in AP_test]\n",
    " \n",
    "        else:\n",
    "            print(\"Model not available:\", model_type)\n",
    "\n",
    "        print(param_str)\n",
    "\n",
    "        # get the test accuracy \n",
    "        cnt = 0\n",
    "        for i in range(len(P_test)):\n",
    "            if P_test[i][0] == y_test_[i][0]:\n",
    "                cnt += 1\n",
    "        accuracy = float(cnt)/len(P_test)\n",
    "        print 'Test accuracy on {0}: {1}'.format(split, accuracy)\n",
    "        accuracies[split_cnt] = accuracy\n",
    "        \n",
    "        # get the train accuracy \n",
    "        cnt = 0\n",
    "        for i in range(len(P_train)):\n",
    "            if P_train[i][0] == y_train_[i][0]:\n",
    "                cnt += 1\n",
    "        train_accuracy = float(cnt)/len(P_train)\n",
    "        print 'Training accuracy on {0}: {1}'.format(split, train_accuracy)\n",
    "        train_accuracies[split_cnt] = train_accuracy\n",
    "        \n",
    "        # save the model\n",
    "        model.save('../models/AP-TCN_v0.3_{0}_{1}_{2}.h5'.format(dataset,split,architecture))\n",
    "        \n",
    "        split_cnt += 1\n",
    "        \n",
    "        if split_cnt >= 1:\n",
    "            break\n",
    "        \n",
    "    print 'Mean test accuracy: {0}'.format(np.mean(accuracies))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3783, 90, 4096)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_m.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# dummy_X_test = np.random.rand(3783,90,4096)\n",
    "dummy_X_test = -1*np.ones((3783,90,4096))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction on test data...\n",
      "All predictions done...\n"
     ]
    }
   ],
   "source": [
    "# predict on the dataset\n",
    "\n",
    "print(\"Prediction on test data...\")\n",
    "AP_test = model.predict(dummy_X_test, verbose=0)\n",
    "print(\"All predictions done...\")\n",
    "\n",
    "# prediction outputs from 0 to 100 while GT contains labels from 1 to 101\n",
    "P_test = [p.argmax(1)+1 for p in AP_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy on Split_01: 0.0103092783505\n"
     ]
    }
   ],
   "source": [
    "# get the test accuracy \n",
    "cnt = 0\n",
    "for i in range(len(P_test)):\n",
    "    if P_test[i][0] == y_test_[i][0]:\n",
    "        cnt += 1\n",
    "accuracy = float(cnt)/len(P_test)\n",
    "print 'Test accuracy on {0}: {1}'.format(split, accuracy)\n",
    "accuracies[split_cnt] = accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3783, 90, 4096)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_m.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_test_m.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3783"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y_test_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([80])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test_[3000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6, 4096)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9537, 90, 4096)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_m.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "base_dir = \"/home/jinchoi/src/rehab/dataset/action/UCF101/\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/jinchoi/src/rehab/action-recog/action_proposal/AP-TCN/code'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "model = load_model(os.getcwd()+'/../models/' + 'AP-TCN_v0.3_UCF101_Split_01_sanity_check1.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train_m = np.load(base_dir + 'AlexNet-fc7-npy/X_train_m_ucf_' + split + '_' + architecture +'.npy')\n",
    "X_test_m = np.load(base_dir + 'AlexNet-fc7-npy/X_test_m_ucf_' + split + '_' + architecture + '.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction on test data...\n",
      "All predictions done...\n"
     ]
    }
   ],
   "source": [
    "print(\"Prediction on test data...\")\n",
    "AP_test = model.predict(X_test_m, verbose=0)\n",
    "print(\"All predictions done...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "P_test = [p.argmax(1)+1 for p in AP_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_test = np.load(base_dir + 'AlexNet-fc7-npy/y_test_ucf_' + split + '_' + architecture +'.npy')\n",
    "y_test_ = [np.array([y_test[i]]) for i in range(len(y_test))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy on Split_01: 0.856991805445\n"
     ]
    }
   ],
   "source": [
    "# get the test accuracy \n",
    "cnt = 0\n",
    "for i in range(len(P_test)):\n",
    "    if P_test[i][0] == y_test_[i][0]:\n",
    "        cnt += 1\n",
    "accuracy = float(cnt)/len(P_test)\n",
    "print 'Test accuracy on {0}: {1}'.format(split, accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:tf-ap]",
   "language": "python",
   "name": "conda-env-tf-ap-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
