{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "from collections import OrderedDict\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from scipy import io as sio\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pylab as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from keras.utils import np_utils\n",
    "\n",
    "# TCN imports\n",
    "import tf_models, ap_datasets, utils, metrics\n",
    "from utils import imshow_\n",
    "\n",
    "# ---------- Directories & User inputs --------------\n",
    "# Location of data/features folder\n",
    "base_dir = os.path.expanduser(\"../\")\n",
    "\n",
    "# Set dataset and action label granularity (if applicable)\n",
    "dataset = [\"50Salads\", \"JIGSAWS\", \"MERL\", \"GTEA\", \"UCF101\"][4]\n",
    "# Set model and parameters\n",
    "# causal or acausal? (If acausal use Bidirectional LSTM)\n",
    "causal = [False, True][0]\n",
    "\n",
    "# How many latent states/nodes per layer of network\n",
    "# Only applicable to the TCNs. The ECCV and LSTM  model suses the first element from this list.\n",
    "n_nodes = [64, 96]\n",
    "conv = {'50Salads':25, \"JIGSAWS\":20, \"MERL\":5, \"GTEA\":25, 'UCF101': 25}[dataset]\n",
    "\n",
    "# Which features for the given dataset\n",
    "features = \"SpatialCNN\"\n",
    "\n",
    "if dataset == \"UCF101\":\n",
    "    base_dir = \"/home/jinchoi/src/rehab/dataset/action/UCF101/\"\n",
    "    feature_type = 'relu7_feat'\n",
    "    model_type = [\"AP-TCN\", \"AP-TCN-SanityCheck\"][0]\n",
    "    video_rate = [25,10][0] \n",
    "    max_len = [90,225][0]\n",
    "    nb_epoch = 40\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================================================================================\n",
      "=================================================================================\n",
      "Training... model_type=AP-TCN, video_rate=15, max_len=90\n",
      "Loading data split...\n",
      "npy files not found. Loading from mat files. This would take a while...\n",
      "Loading the training data: 9%"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-d14ecba00dbf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     40\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m                     \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"npy files not found. Loading from mat files. This would take a while...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m                     \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_split_ucf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvideo_rate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeature_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m                     \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase_dir\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'AlexNet-fc7-npy/X_train_ucf_'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msplit\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'_'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvideo_rate\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'f_to1f'\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m'.npy'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m                     \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase_dir\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'AlexNet-fc7-npy/y_train_ucf_'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msplit\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'_'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvideo_rate\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'f_to1f'\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m'.npy'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/jinchoi/src/rehab/action-recog/action_proposal/AP-TCN/code/ap_datasets.py\u001b[0m in \u001b[0;36mload_split_ucf\u001b[0;34m(self, split, feature_type, sample_rate)\u001b[0m\n\u001b[1;32m    237\u001b[0m             \u001b[0mfeat_len\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_tmp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfeature_type\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    238\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_tmp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfeature_type\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# loop over time steps\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 239\u001b[0;31m                 \u001b[0mtemp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_tmp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfeature_type\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeat_len\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    240\u001b[0m                 \u001b[0mtmp_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtemp\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    241\u001b[0m             \u001b[0mX_train\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m[\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtmp_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for model_type in [\"AP-TCN\"]:#, \"AP-TCN-SanityCheck\"]:\n",
    "    for video_rate in [15]:#, 10]:\n",
    "#         if model_type == \"AP-TCN\" and video_rate == 25 :\n",
    "#             continue\n",
    "        for max_len in [90]:#, 225]:\n",
    "#             if video_rate == 10 and max_len == 90:\n",
    "#                 continue\n",
    "            print('=================================================================================')\n",
    "            print('=================================================================================')\n",
    "            print('Training... model_type={0}, video_rate={1}, max_len={2}'.format(model_type, video_rate, max_len))\n",
    "            # Initialize dataset loader & metrics\n",
    "            if dataset == 'UCF101':\n",
    "                data = ap_datasets.Dataset(dataset, base_dir, feature_type='fc7')\n",
    "            else:\n",
    "                data = ap_datasets.Dataset(dataset, base_dir)\n",
    "\n",
    "            n_classes = data.n_classes\n",
    "            test_accuracies = list()\n",
    "            test_losses = list()\n",
    "            split_cnt = 0\n",
    "\n",
    "            # Load data for each split\n",
    "            for split in data.splits:\n",
    "                if dataset != 'UCF101':\n",
    "                    feature_type = \"A\" if model_type != \"SVM\" else \"X\"\n",
    "\n",
    "                # Load the feature files\n",
    "                print(\"Loading data split...\")\n",
    "                # If there exist .npy files, load them\n",
    "                if ( os.path.exists(base_dir + 'AlexNet-fc7-npy/X_train_ucf_' + split + '_' + str(video_rate) + 'f_to1f' +'.npy') and\n",
    "                     os.path.exists(base_dir + 'AlexNet-fc7-npy/y_train_ucf_' + split + '_' + str(video_rate) + 'f_to1f' +'.npy') and\n",
    "                     os.path.exists(base_dir + 'AlexNet-fc7-npy/X_test_ucf_' + split + '_' + str(video_rate) + 'f_to1f' +'.npy') and\n",
    "                     os.path.exists(base_dir + 'AlexNet-fc7-npy/y_test_ucf_' + split + '_' + str(video_rate) + 'f_to1f' +'.npy') ):\n",
    "                    print(\"npy files found.\")\n",
    "                    X_train = np.load(base_dir + 'AlexNet-fc7-npy/X_train_ucf_' + split + '_' + str(video_rate) + 'f_to1f' +'.npy');\n",
    "                    y_train = np.load(base_dir + 'AlexNet-fc7-npy/y_train_ucf_' + split + '_' + str(video_rate) + 'f_to1f' +'.npy');\n",
    "                    X_test = np.load(base_dir + 'AlexNet-fc7-npy/X_test_ucf_' + split + '_' + str(video_rate) + 'f_to1f' +'.npy');\n",
    "                    y_test = np.load(base_dir + 'AlexNet-fc7-npy/y_test_ucf_' + split + '_' + str(video_rate) + 'f_to1f' +'.npy');\n",
    "                # If there are no .npy files, load .mat files and generate the numpy features\n",
    "                else:\n",
    "                    print(\"npy files not found. Loading from mat files. This would take a while...\")\n",
    "                    X_train, y_train, X_test, y_test = data.load_split_ucf(split=split, sample_rate=video_rate, feature_type=feature_type)\n",
    "                    np.save(base_dir + 'AlexNet-fc7-npy/X_train_ucf_' + split + '_' + str(video_rate) + 'f_to1f' +'.npy', X_train);\n",
    "                    np.save(base_dir + 'AlexNet-fc7-npy/y_train_ucf_' + split + '_' + str(video_rate) + 'f_to1f' +'.npy', y_train);\n",
    "                    np.save(base_dir + 'AlexNet-fc7-npy/X_test_ucf_' + split + '_' + str(video_rate) + 'f_to1f' +'.npy', X_test);\n",
    "                    np.save(base_dir + 'AlexNet-fc7-npy/y_test_ucf_' + split + '_' + str(video_rate) + 'f_to1f' +'.npy', y_test);\n",
    "                print(\"Loading done.\")\n",
    "\n",
    "                n_feat = data.n_features = X_train[0].shape[1]\n",
    "\n",
    "                # --------- ICCV model ----------\n",
    "                # for train a softmax classfier, we need one-hot encoded labels, from 0 to 100\n",
    "                Y_train = [np_utils.to_categorical(y-1, n_classes) for y in y_train]\n",
    "                Y_train_ = np.array(Y_train)\n",
    "                y_train_ = [np.array([y_train[i]]) for i in range(len(y_train))] # no need\n",
    "\n",
    "                # for train a softmax classfier, we need one-hot encoded labels, from 0 to 100\n",
    "                Y_test = [np_utils.to_categorical(y-1, n_classes) for y in y_test]\n",
    "                Y_test_ = np.array(Y_test)\n",
    "                y_test_ = [np.array([y_test[i]]) for i in range(len(y_test))] # no need\n",
    "\n",
    "                # In order process batches simultaneously all data needs to be of the same length\n",
    "                # So make all same length and mask out the ends of each.\n",
    "                n_layers = len(n_nodes)\n",
    "\n",
    "                print(\"Start data masking...\")\n",
    "                X_train_m, _ = utils.mask_data_one_tensor(X_train, max_len, mask_value=-1)\n",
    "                X_test_m,  _ = utils.mask_data_one_tensor(X_test,  max_len, mask_value=-1)\n",
    "                print(\"Data masking done.\")\n",
    "\n",
    "                ###  Random shuffle of the training data and corresponding labels\n",
    "                ###  This is important since the Keras model.fit function does shuffle\n",
    "                ###  after sampling the last portion of the training data !!!\n",
    "                print(\"Shuffling the training data...\")\n",
    "                rand_ind = np.random.permutation(len(X_train_m))\n",
    "                X_train_m_shuffle = X_train_m[rand_ind,:,:]\n",
    "                Y_train_shuffle   = np.array( [Y_train[rand_ind[i]] for i in range(len(Y_train))] )\n",
    "                print(\"Shuffling done\")\n",
    "\n",
    "                if model_type == \"AP-TCN\":\n",
    "                    print('Training AP-TCN...')\n",
    "                    model, param_str = tf_models.AP_TCN(n_nodes, conv, n_classes, n_feat, max_len, causal=causal, activation='norm_relu', return_param_str=True)\n",
    "                elif model_type == \"AP-TCN-SanityCheck\":\n",
    "                    model, param_str = tf_models.AP_TCN_SanityCheck(n_nodes, conv, n_classes, n_feat, max_len, causal=causal,\n",
    "                                                        activation='norm_relu', return_param_str=True)\n",
    "\n",
    "                #model.summary()\n",
    "                M_train2 = np.array([[1] for i in range(len(X_train))])\n",
    "\n",
    "                # fitting a model\n",
    "                print('model_type={0}, video_rate={1}, max_len={2}, nb_epoch={3}, split={4}'.format(model_type,video_rate,max_len,nb_epoch,split))\n",
    "                model.fit(X_train_m_shuffle, Y_train_shuffle, nb_epoch=nb_epoch, batch_size=8, verbose=1, shuffle=True, sample_weight=M_train2)\n",
    "\n",
    "                print(param_str)\n",
    "\n",
    "                print('Evaluation on blind test dataset')\n",
    "                [test_loss, test_accuracy] = model.evaluate(X_test_m, Y_test_)\n",
    "                test_accuracies.append(test_accuracy)\n",
    "                test_losses.append(test_accuracy)\n",
    "                print('Test Accuracy: {0}, Test Loss:{1}'.format(test_accuracy, test_loss))\n",
    "\n",
    "                # save the model\n",
    "                model.save('../models/{0}_r{1}_l{2}_{3}_{4}_epoch{5}.h5'.format(model_type, video_rate, max_len, dataset, split, nb_epoch))\n",
    "\n",
    "                split_cnt += 1\n",
    "                if split_cnt >= 1:\n",
    "                    break;\n",
    "\n",
    "            print 'Mean test accuracy: {0}'.format(np.mean(test_accuracies))\n",
    "            print('=================================================================================')\n",
    "            print('=================================================================================')"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:tf-ap]",
   "language": "python",
   "name": "conda-env-tf-ap-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
