{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "from collections import OrderedDict\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from scipy import io as sio\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from keras.utils import np_utils\n",
    "\n",
    "# TCN imports \n",
    "import tf_models, ap_datasets, utils, metrics\n",
    "from utils import imshow_\n",
    "\n",
    "# ---------- Directories & User inputs --------------\n",
    "# Location of data/features folder\n",
    "base_dir = os.path.expanduser(\"../\")\n",
    "\n",
    "# Set dataset and action label granularity (if applicable)\n",
    "dataset = [\"50Salads\", \"JIGSAWS\", \"MERL\", \"GTEA\", \"UCF101\"][4]\n",
    "# Set model and parameters\n",
    "model_type = [\"AP-TCN\"][0]\n",
    "# causal or acausal? (If acausal use Bidirectional LSTM)\n",
    "causal = [False, True][0]\n",
    "\n",
    "# How many latent states/nodes per layer of network\n",
    "# Only applicable to the TCNs. The ECCV and LSTM  model suses the first element from this list.\n",
    "n_nodes = [64, 96]\n",
    "nb_epoch = 30 #50\n",
    "video_rate = 3\n",
    "conv = {'50Salads':25, \"JIGSAWS\":20, \"MERL\":5, \"GTEA\":25, 'UCF101': 25}[dataset]\n",
    "\n",
    "# Which features for the given dataset\n",
    "features = \"SpatialCNN\"\n",
    "\n",
    "if dataset == \"UCF101\":\n",
    "    base_dir = \"/home/jinchoi/src/rehab/dataset/action/UCF101/\"\n",
    "    feature_type = 'relu7_feat'\n",
    "    video_rate = 10 #25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data split...\n",
      "npy files not found. Loading from mat files. This would take a while...\n",
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "1000\n",
      "1100\n",
      "1200\n",
      "1300\n",
      "1400\n",
      "1500\n",
      "1600\n",
      "1700\n",
      "1800\n",
      "1900\n",
      "2000\n",
      "2100\n",
      "2200\n",
      "2300\n",
      "2400\n",
      "2500\n",
      "2600\n",
      "2700\n",
      "2800\n",
      "2900\n",
      "3000\n",
      "3100\n",
      "3200\n",
      "3300\n",
      "3400\n",
      "3500\n",
      "3600\n",
      "3700\n",
      "3800\n",
      "3900\n",
      "4000\n",
      "4100\n",
      "4200\n",
      "4300\n",
      "4400\n",
      "4500\n",
      "4600\n",
      "4700\n",
      "4800\n",
      "4900\n",
      "5000\n",
      "5100\n",
      "5200\n",
      "5300\n",
      "5400\n",
      "5500\n",
      "5600\n",
      "5700\n",
      "5800\n",
      "5900\n",
      "6000\n",
      "6100\n",
      "6200\n",
      "6300\n",
      "6400\n",
      "6500\n",
      "6600\n",
      "6700\n",
      "6800\n",
      "6900\n",
      "7000\n",
      "7100\n",
      "7200\n",
      "7300\n",
      "7400\n",
      "7500\n",
      "7600\n",
      "7700\n",
      "7800\n",
      "7900\n",
      "8000\n",
      "8100\n",
      "8200\n",
      "8300\n",
      "8400\n",
      "8500\n",
      "8600\n",
      "8700\n",
      "8800\n",
      "8900\n",
      "9000\n",
      "9100\n",
      "9200\n",
      "9300\n",
      "9400\n",
      "9500\n",
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "1000\n",
      "1100\n",
      "1200\n",
      "1300\n",
      "1400\n",
      "1500\n",
      "1600\n",
      "1700\n",
      "1800\n",
      "1900\n",
      "2000\n",
      "2100\n",
      "2200\n",
      "2300\n",
      "2400\n",
      "2500\n",
      "2600\n",
      "2700\n",
      "2800\n",
      "2900\n",
      "3000\n",
      "3100\n",
      "3200\n",
      "3300\n",
      "3400\n",
      "3500\n",
      "3600\n",
      "3700\n",
      "Loading done.\n",
      "Start data masking...\n",
      "Data masking done.\n",
      "Shuffling the training data...\n",
      "Shuffling done\n",
      "Training AP-TCN...\n",
      "Train on 7629 samples, validate on 1908 samples\n",
      "Epoch 1/30\n",
      "7629/7629 [==============================] - 25s - loss: 1.9633 - acc: 0.5082 - val_loss: 0.6039 - val_acc: 0.8370\n",
      "Epoch 2/30\n",
      "7629/7629 [==============================] - 25s - loss: 0.3484 - acc: 0.8862 - val_loss: 0.2272 - val_acc: 0.9182\n",
      "Epoch 3/30\n",
      "7629/7629 [==============================] - 26s - loss: 0.1721 - acc: 0.9418 - val_loss: 0.2075 - val_acc: 0.9544\n",
      "Epoch 4/30\n",
      "7629/7629 [==============================] - 25s - loss: 0.1104 - acc: 0.9617 - val_loss: 0.1594 - val_acc: 0.9460\n",
      "Epoch 5/30\n",
      "7629/7629 [==============================] - 25s - loss: 0.0839 - acc: 0.9692 - val_loss: 0.1018 - val_acc: 0.9486\n",
      "Epoch 6/30\n",
      "7629/7629 [==============================] - 25s - loss: 0.0679 - acc: 0.9729 - val_loss: 0.0405 - val_acc: 0.9811\n",
      "Epoch 7/30\n",
      "7629/7629 [==============================] - 25s - loss: 0.0560 - acc: 0.9759 - val_loss: 0.0897 - val_acc: 0.9670\n",
      "Epoch 8/30\n",
      "7629/7629 [==============================] - 25s - loss: 0.0479 - acc: 0.9786 - val_loss: 0.0281 - val_acc: 0.9806\n",
      "Epoch 9/30\n",
      "7629/7629 [==============================] - 25s - loss: 0.0429 - acc: 0.9799 - val_loss: 0.0400 - val_acc: 0.9817\n",
      "Epoch 10/30\n",
      "7629/7629 [==============================] - 25s - loss: 0.0349 - acc: 0.9839 - val_loss: 0.0352 - val_acc: 0.9848\n",
      "Epoch 11/30\n",
      "7629/7629 [==============================] - 25s - loss: 0.0363 - acc: 0.9839 - val_loss: 0.0207 - val_acc: 0.9916\n",
      "Epoch 12/30\n",
      "7629/7629 [==============================] - 25s - loss: 0.0308 - acc: 0.9852 - val_loss: 0.0213 - val_acc: 0.9911\n",
      "Epoch 13/30\n",
      "7629/7629 [==============================] - 25s - loss: 0.0255 - acc: 0.9875 - val_loss: 0.0911 - val_acc: 0.9696\n",
      "Epoch 14/30\n",
      "7629/7629 [==============================] - 25s - loss: 0.0225 - acc: 0.9889 - val_loss: 0.0194 - val_acc: 0.9874\n",
      "Epoch 15/30\n",
      "7629/7629 [==============================] - 25s - loss: 0.0216 - acc: 0.9887 - val_loss: 0.0183 - val_acc: 0.9890\n",
      "Epoch 16/30\n",
      "7629/7629 [==============================] - 25s - loss: 0.0207 - acc: 0.9900 - val_loss: 0.0284 - val_acc: 0.9790\n",
      "Epoch 17/30\n",
      "7629/7629 [==============================] - 25s - loss: 0.0177 - acc: 0.9910 - val_loss: 0.0181 - val_acc: 0.9916\n",
      "Epoch 18/30\n",
      "7629/7629 [==============================] - 26s - loss: 0.0170 - acc: 0.9910 - val_loss: 0.0182 - val_acc: 0.9906\n",
      "Epoch 19/30\n",
      "7629/7629 [==============================] - 25s - loss: 0.0137 - acc: 0.9924 - val_loss: 0.0227 - val_acc: 0.9869\n",
      "Epoch 20/30\n",
      "7629/7629 [==============================] - 25s - loss: 0.0127 - acc: 0.9936 - val_loss: 0.0124 - val_acc: 0.9979\n",
      "Epoch 21/30\n",
      "7629/7629 [==============================] - 25s - loss: 0.0112 - acc: 0.9949 - val_loss: 0.0140 - val_acc: 0.9890\n",
      "Epoch 22/30\n",
      "7629/7629 [==============================] - 25s - loss: 0.0119 - acc: 0.9945 - val_loss: 0.0061 - val_acc: 0.9990\n",
      "Epoch 23/30\n",
      "7629/7629 [==============================] - 25s - loss: 0.0089 - acc: 0.9959 - val_loss: 0.0141 - val_acc: 0.9916\n",
      "Epoch 24/30\n",
      "7629/7629 [==============================] - 25s - loss: 0.0094 - acc: 0.9961 - val_loss: 0.0067 - val_acc: 0.9984\n",
      "Epoch 25/30\n",
      "7629/7629 [==============================] - 25s - loss: 0.0077 - acc: 0.9967 - val_loss: 0.0148 - val_acc: 0.9900\n",
      "Epoch 26/30\n",
      "7629/7629 [==============================] - 25s - loss: 0.0083 - acc: 0.9959 - val_loss: 0.0042 - val_acc: 0.9984\n",
      "Epoch 27/30\n",
      "7629/7629 [==============================] - 26s - loss: 0.0069 - acc: 0.9966 - val_loss: 0.0182 - val_acc: 0.9890\n",
      "Epoch 28/30\n",
      "7629/7629 [==============================] - 24s - loss: 0.0059 - acc: 0.9969 - val_loss: 7.2754e-04 - val_acc: 1.0000\n",
      "Epoch 29/30\n",
      "7629/7629 [==============================] - 24s - loss: 0.0057 - acc: 0.9974 - val_loss: 7.3806e-04 - val_acc: 1.0000\n",
      "Epoch 30/30\n",
      "7629/7629 [==============================] - 24s - loss: 0.0048 - acc: 0.9976 - val_loss: 0.0039 - val_acc: 0.9990\n",
      "AP-TCN_C25_L2\n",
      "Evaluation on blind test dataset\n",
      "3783/3783 [==============================] - 7s     \n",
      "Test Accuracy: 0.999206978588, Test Loss:0.00399789566902\n",
      "Mean test accuracy: 0.999206978588\n"
     ]
    }
   ],
   "source": [
    "#from IPython.core.debugger import Pdb # for debugging, jinchoi@vt.edu\n",
    "#pdb = Pdb()\n",
    "\n",
    "# Initialize dataset loader & metrics\n",
    "if dataset == 'UCF101':\n",
    "    data = ap_datasets.Dataset(dataset, base_dir, feature_type='fc7')\n",
    "else:\n",
    "    data = ap_datasets.Dataset(dataset, base_dir)\n",
    "\n",
    "n_classes = data.n_classes\n",
    "split_cnt = 0\n",
    "test_accuracies = list()\n",
    "test_losses = list()\n",
    "\n",
    "# Load data for each split\n",
    "for split in data.splits:\n",
    "    if dataset != 'UCF101':\n",
    "        feature_type = \"A\" if model_type != \"SVM\" else \"X\"\n",
    "\n",
    "    # Load the feature files\n",
    "    print(\"Loading data split...\")\n",
    "    # If there exist .npy files, load them\n",
    "    if ( os.path.exists(base_dir + 'AlexNet-fc7-npy/X_train_ucf_' + split + '_' + str(video_rate) + 'f_to1f' +'.npy') and \n",
    "         os.path.exists(base_dir + 'AlexNet-fc7-npy/y_train_ucf_' + split + '_' + str(video_rate) + 'f_to1f' +'.npy') and\n",
    "         os.path.exists(base_dir + 'AlexNet-fc7-npy/X_test_ucf_' + split + '_' + str(video_rate) + 'f_to1f' +'.npy') and\n",
    "         os.path.exists(base_dir + 'AlexNet-fc7-npy/y_test_ucf_' + split + '_' + str(video_rate) + 'f_to1f' +'.npy') ):\n",
    "        print(\"npy files found.\")\n",
    "        X_train = np.load(base_dir + 'AlexNet-fc7-npy/X_train_ucf_' + split + '_' + str(video_rate) + 'f_to1f' +'.npy');\n",
    "        y_train = np.load(base_dir + 'AlexNet-fc7-npy/y_train_ucf_' + split + '_' + str(video_rate) + 'f_to1f' +'.npy');\n",
    "        X_test = np.load(base_dir + 'AlexNet-fc7-npy/X_test_ucf_' + split + '_' + str(video_rate) + 'f_to1f' +'.npy');\n",
    "        y_test = np.load(base_dir + 'AlexNet-fc7-npy/y_test_ucf_' + split + '_' + str(video_rate) + 'f_to1f' +'.npy');\n",
    "    # If there are no .npy files, load .mat files and generate the numpy features\n",
    "    else:\n",
    "        print(\"npy files not found. Loading from mat files. This would take a while...\")\n",
    "        X_train, y_train, X_test, y_test = data.load_split(features, split=split, \n",
    "                                                            sample_rate=video_rate, \n",
    "                                                            feature_type=feature_type)\n",
    "        np.save(base_dir + 'AlexNet-fc7-npy/X_train_ucf_' + split + '_' + str(video_rate) + 'f_to1f' +'.npy', X_train);\n",
    "        np.save(base_dir + 'AlexNet-fc7-npy/y_train_ucf_' + split + '_' + str(video_rate) + 'f_to1f' +'.npy', y_train);\n",
    "        np.save(base_dir + 'AlexNet-fc7-npy/X_test_ucf_' + split + '_' + str(video_rate) + 'f_to1f' +'.npy', X_test);\n",
    "        np.save(base_dir + 'AlexNet-fc7-npy/y_test_ucf_' + split + '_' + str(video_rate) + 'f_to1f' +'.npy', y_test);\n",
    "    print(\"Loading done.\")\n",
    "\n",
    "    # for later use\n",
    "#         pca = PCA(n_components = 32)\n",
    "#         pca.fit(X_train[0])\n",
    "    n_feat = data.n_features = X_train[0].shape[1]\n",
    "\n",
    "    # --------- CVPR model ----------\n",
    "    if model_type in [\"ED-TCN\",\"AP-TCN\"]:\n",
    "        # for train a softmax classfier, we need one-hot encoded labels, from 0 to 100\n",
    "        Y_train = [np_utils.to_categorical(y-1, n_classes) for y in y_train]  \n",
    "        Y_train_ = np.array(Y_train)\n",
    "        y_train_ = [np.array([y_train[i]]) for i in range(len(y_train))] # no need\n",
    "\n",
    "        # for train a softmax classfier, we need one-hot encoded labels, from 0 to 100\n",
    "        Y_test = [np_utils.to_categorical(y-1, n_classes) for y in y_test]\n",
    "        Y_test_ = np.array(Y_test)\n",
    "        y_test_ = [np.array([y_test[i]]) for i in range(len(y_test))] # no need\n",
    "\n",
    "        # In order process batches simultaneously all data needs to be of the same length\n",
    "        # So make all same length and mask out the ends of each.\n",
    "        n_layers = len(n_nodes)\n",
    "        max_len = 225 #90 # this should be elaborated\n",
    "\n",
    "        print(\"Start data masking...\")\n",
    "        X_train_m, _ = utils.mask_data_one_tensor(X_train, max_len, mask_value=-1)\n",
    "        X_test_m,  _ = utils.mask_data_one_tensor(X_test,  max_len, mask_value=-1)\n",
    "        print(\"Data masking done.\")\n",
    "\n",
    "        ###  Random shuffle of the training data and corresponding labels\n",
    "        ###  This is important since the Keras model.fit function does shuffle \n",
    "        ###  after sampling the last portion of the training data !!!\n",
    "        print(\"Shuffling the training data...\")\n",
    "        rand_ind = np.random.permutation(len(X_train_m))\n",
    "        X_train_m_shuffle = X_train_m[rand_ind,:,:]\n",
    "        Y_train_shuffle   = np.array( [Y_train[rand_ind[i]] for i in range(len(Y_train))] )\n",
    "        print(\"Shuffling done\")\n",
    "\n",
    "        if model_type == \"AP-TCN\":\n",
    "            print('Training AP-TCN...')\n",
    "#             model, param_str = tf_models.AP_TCN(n_nodes, conv, n_classes, n_feat, max_len, causal=causal, \n",
    "#                                     activation='norm_relu', return_param_str=True)       \n",
    "            model, param_str = tf_models.AP_TCN_SanityCheck(n_nodes, conv, n_classes, n_feat, max_len, causal=causal, \n",
    "                                                activation='norm_relu', return_param_str=True)               \n",
    "\n",
    "        #model.summary()\n",
    "        M_train2 = np.array([[1] for i in range(len(X_train))])\n",
    "\n",
    "        # fitting a model\n",
    "        model.fit(X_train_m_shuffle, Y_train_shuffle, nb_epoch=nb_epoch, batch_size=8, verbose=1, validation_split=0.2, shuffle=True, sample_weight=M_train2) \n",
    "        #model.fit(X_train_m, Y_train_, nb_epoch=nb_epoch, batch_size=8, verbose=1, validation_split=0.1, shuffle=True, sample_weight=M_train2) \n",
    "\n",
    "#             # predict on the dataset\n",
    "#             print(\"Prediction on training data...\")            \n",
    "#             AP_train = model.predict(X_train_m, verbose=0)\n",
    "#             print(\"Prediction on test data...\")\n",
    "#             AP_test = model.predict(X_test_m, verbose=0)\n",
    "#             print(\"All predictions done...\")\n",
    "\n",
    "#             # prediction outputs from 0 to 100 while GT contains labels from 1 to 101\n",
    "#             P_train = [p.argmax(1)+1 for p in AP_train] \n",
    "#             P_test = [p.argmax(1)+1 for p in AP_test]\n",
    "\n",
    "    else:\n",
    "        print(\"Model not available:\", model_type)\n",
    "\n",
    "    print(param_str)\n",
    "\n",
    "#         # get the test accuracy \n",
    "#         cnt = 0\n",
    "#         for i in range(len(P_test)):\n",
    "#             if P_test[i][0] == y_test_[i][0]:\n",
    "#                 cnt += 1\n",
    "#         accuracy = float(cnt)/len(P_test)\n",
    "#         print 'Test accuracy on {0}: {1}'.format(split, accuracy)\n",
    "#         accuracies[split_cnt] = accuracy\n",
    "\n",
    "#         # get the train accuracy \n",
    "#         cnt = 0\n",
    "#         for i in range(len(P_train)):\n",
    "#             if P_train[i][0] == y_train_[i][0]:\n",
    "#                 cnt += 1\n",
    "#         train_accuracy = float(cnt)/len(P_train)\n",
    "#         print 'Training accuracy on {0}: {1}'.format(split, train_accuracy)\n",
    "#         train_accuracies[split_cnt] = train_accuracy\n",
    "\n",
    "    print('Evaluation on blind test dataset')\n",
    "    [test_loss, test_accuracy] = model.evaluate(X_test_m, Y_test_)\n",
    "    test_accuracies.append(test_accuracy)\n",
    "    test_losses.append(test_accuracy)\n",
    "    print('Test Accuracy: {0}, Test Loss:{1}'.format(test_accuracy, test_loss))\n",
    "\n",
    "    # save the model\n",
    "    model.save('../models/AP-TCN_v0.3_{0}_{1}_epoch{2}.h5'.format(dataset,split,nb_epoch))\n",
    "\n",
    "    split_cnt += 1\n",
    "\n",
    "    if split_cnt >= 1:\n",
    "        break\n",
    "\n",
    "print 'Mean test accuracy: {0}'.format(np.mean(test_accuracies))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:tf-ap]",
   "language": "python",
   "name": "conda-env-tf-ap-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
