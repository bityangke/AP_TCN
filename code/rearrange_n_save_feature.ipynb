{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data split...\n",
      "npy files not found. Loading from mat files. This would take a while...\n",
      "0\n",
      "200\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "from collections import OrderedDict\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from scipy import io as sio\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from keras.utils import np_utils\n",
    "\n",
    "# TCN imports \n",
    "import tf_models, ap_datasets, utils, metrics\n",
    "from utils import imshow_\n",
    "\n",
    "# ---------- Directories & User inputs --------------\n",
    "# Location of data/features folder\n",
    "base_dir = os.path.expanduser(\"../\")\n",
    "\n",
    "# Set dataset and action label granularity (if applicable)\n",
    "dataset = [\"50Salads\", \"JIGSAWS\", \"MERL\", \"GTEA\", \"UCF101\"][4]\n",
    "# Set model and parameters\n",
    "model_type = [\"AP-TCN\", \"AP-TCN-SanityCheck\"][0]\n",
    "# causal or acausal? (If acausal use Bidirectional LSTM)\n",
    "causal = [False, True][0]\n",
    "\n",
    "# How many latent states/nodes per layer of network\n",
    "# Only applicable to the TCNs. The ECCV and LSTM  model suses the first element from this list.\n",
    "n_nodes = [64, 96]\n",
    "nb_epoch = 40 #50\n",
    "conv = {'50Salads':25, \"JIGSAWS\":20, \"MERL\":5, \"GTEA\":25, 'UCF101': 25}[dataset]\n",
    "\n",
    "# Which features for the given dataset\n",
    "features = \"SpatialCNN\"\n",
    "\n",
    "if dataset == \"UCF101\":\n",
    "    base_dir = \"/home/jinchoi/src/rehab/dataset/action/UCF101/\"\n",
    "    feature_type = 'relu7_feat'\n",
    "    video_rate = 10\n",
    "    \n",
    "feature_type='pool5_feat'\n",
    "data = ap_datasets.Dataset(dataset, base_dir, feature_type='pool5')\n",
    "for split in data.splits:\n",
    "    if dataset != 'UCF101':\n",
    "        feature_type = \"A\" if model_type != \"SVM\" else \"X\"\n",
    "\n",
    "    # Load the feature files\n",
    "    print(\"Loading data split...\")\n",
    "    # If there exist .npy files, load them\n",
    "    if ( os.path.exists(base_dir + 'AlexNet-pool5-npy/X_train_ucf_' + split + '_' + str(video_rate) + 'f_to1f' +'.npy') and \n",
    "         os.path.exists(base_dir + 'AlexNet-pool5-npy/y_train_ucf_' + split + '_' + str(video_rate) + 'f_to1f' +'.npy') and\n",
    "         os.path.exists(base_dir + 'AlexNet-pool5-npy/X_test_ucf_' + split + '_' + str(video_rate) + 'f_to1f' +'.npy') and\n",
    "         os.path.exists(base_dir + 'AlexNet-pool5-npy/y_test_ucf_' + split + '_' + str(video_rate) + 'f_to1f' +'.npy') ):\n",
    "        print(\"npy files found.\")\n",
    "        X_train = np.load(base_dir + 'AlexNet-pool5-npy/X_train_ucf_' + split + '_' + str(video_rate) + 'f_to1f' +'.npy');\n",
    "        y_train = np.load(base_dir + 'AlexNet-pool5-npy/y_train_ucf_' + split + '_' + str(video_rate) + 'f_to1f' +'.npy');\n",
    "        X_test = np.load(base_dir + 'AlexNet-pool5-npy/X_test_ucf_' + split + '_' + str(video_rate) + 'f_to1f' +'.npy');\n",
    "        y_test = np.load(base_dir + 'AlexNet-pool5-npy/y_test_ucf_' + split + '_' + str(video_rate) + 'f_to1f' +'.npy');\n",
    "    # If there are no .npy files, load .mat files and generate the numpy features\n",
    "    else:\n",
    "        print(\"npy files not found. Loading from mat files. This would take a while...\")\n",
    "        X_train, y_train, X_test, y_test = data.load_split(features, split=split, \n",
    "                                                            sample_rate=video_rate, \n",
    "                                                            feature_type=feature_type)\n",
    "        np.save(base_dir + 'AlexNet-pool5-npy/X_train_ucf_' + split + '_' + str(video_rate) + 'f_to1f' +'.npy', X_train);\n",
    "        np.save(base_dir + 'AlexNet-pool5-npy/y_train_ucf_' + split + '_' + str(video_rate) + 'f_to1f' +'.npy', y_train);\n",
    "        np.save(base_dir + 'AlexNet-pool5-npy/X_test_ucf_' + split + '_' + str(video_rate) + 'f_to1f' +'.npy', X_test);\n",
    "        np.save(base_dir + 'AlexNet-pool5-npy/y_test_ucf_' + split + '_' + str(video_rate) + 'f_to1f' +'.npy', y_test);\n",
    "    print(\"Loading done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tf-ap]",
   "language": "python",
   "name": "conda-env-tf-ap-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
